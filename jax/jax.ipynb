{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, value_and_grad\n",
    "from jax import random\n",
    "\n",
    "# Generate key which is used to generate random numbers\n",
    "key = random.PRNGKey(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 8 ms\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 30.3 ms\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 9.98 ms\n"
     ]
    }
   ],
   "source": [
    "# Generate a random matrix\n",
    "x = random.uniform(key, (1000, 1000))\n",
    "# Compare running times of 3 different matrix multiplications\n",
    "%time y = np.dot(x, x)\n",
    "%time y = jnp.dot(x, x)\n",
    "%time y = jnp.dot(x, x).block_until_ready()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    \"\"\" Rectified Linear Unit (ReLU) activation function \"\"\"\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "jit_ReLU = jit(ReLU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 21.8 ms\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 26.4 ms\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time out = ReLU(x).block_until_ready()\n",
    "# Call jitted version to compile for evaluation time!\n",
    "%time jit_ReLU(x).block_until_ready()\n",
    "%time out = jit_ReLU(x).block_until_ready()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jax Grad:  1.0\n",
      "FD Gradient: 0.99998707\n"
     ]
    }
   ],
   "source": [
    "def FiniteDiffGrad(x):\n",
    "    \"\"\" Compute the finite difference derivative approx for the ReLU\"\"\"\n",
    "    return jnp.array((ReLU(x + 1e-3) - ReLU(x - 1e-3)) / (2 * 1e-3))\n",
    "\n",
    "# Compare the Jax gradient with a finite difference approximation\n",
    "print(\"Jax Grad: \", jit(grad(jit(ReLU)))(2.))\n",
    "print(\"FD Gradient:\", FiniteDiffGrad(2.))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dim = 32\n",
    "feature_dim = 100\n",
    "hidden_dim = 512\n",
    "\n",
    "# Generate a batch of vectors to process\n",
    "X = random.normal(key, (batch_dim, feature_dim))\n",
    "\n",
    "# Generate Gaussian weights and biases\n",
    "params = [random.normal(key, (hidden_dim, feature_dim)),\n",
    "          random.normal(key, (hidden_dim, ))]\n",
    "\n",
    "def relu_layer(params, x):\n",
    "    \"\"\" Simple ReLu layer for single sample \"\"\"\n",
    "    return ReLU(jnp.dot(params[0], x) + params[1])\n",
    "\n",
    "def batch_version_relu_layer(params, x):\n",
    "    \"\"\" Error prone batch version \"\"\"\n",
    "    return ReLU(jnp.dot(X, params[0].T) + params[1])\n",
    "\n",
    "def vmap_relu_layer(params, x):\n",
    "    \"\"\" vmap version of the ReLU layer \"\"\"\n",
    "    return jit(vmap(relu_layer, in_axes=(None, 0), out_axes=0))\n",
    "\n",
    "out = jnp.stack([relu_layer(params, X[i, :]) for i in range(X.shape[0])])\n",
    "out = batch_version_relu_layer(params, X)\n",
    "out = vmap_relu_layer(params, X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron in JAX\n",
    "This section will implement an MLP using JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some additional JAX and dataloader helpers\n",
    "from jax.scipy.special import logsumexp\n",
    "from jax.example_libraries import optimizers\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import time\n",
    "# from helpers import plot_mnist_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:08<00:00, 1180489.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 11516989.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:01<00:00, 1236996.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the PyTorch Data Loader for the training & test set\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_mlp(sizes, key):\n",
    "    \"\"\" Initialize the weights of all layers of a linear layer network \"\"\"\n",
    "    keys = random.split(key, len(sizes))\n",
    "    # Initialize a single layer with Gaussian weights -  helper function\n",
    "    def initialize_layer(m, n, key, scale=1e-2):\n",
    "        w_key, b_key = random.split(key)\n",
    "        return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
    "    return [initialize_layer(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
    "\n",
    "layer_sizes = [784, 512, 512, 10]\n",
    "# Return a list of tuples of layer weights\n",
    "params = initialize_mlp(layer_sizes, key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(params, in_array):\n",
    "    \"\"\" Compute the forward pass for each example individually \"\"\"\n",
    "    activations = in_array\n",
    "\n",
    "    # Loop over the ReLU hidden layers\n",
    "    for w, b in params[:-1]:\n",
    "        activations = relu_layer([w, b], activations)\n",
    "\n",
    "    # Perform final trafo to logits\n",
    "    final_w, final_b = params[-1]\n",
    "    logits = jnp.dot(final_w, activations) + final_b\n",
    "    return logits - logsumexp(logits)\n",
    "\n",
    "# Make a batched version of the `predict` function\n",
    "batch_forward = vmap(forward_pass, in_axes=(None, 0), out_axes=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x, k, dtype=jnp.float32):\n",
    "    \"\"\"Create a one-hot encoding of x of size k \"\"\"\n",
    "    return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "\n",
    "def loss(params, in_arrays, targets):\n",
    "    \"\"\" Compute the multi-class cross-entropy loss \"\"\"\n",
    "    preds = batch_forward(params, in_arrays)\n",
    "    return -jnp.sum(preds * targets)\n",
    "\n",
    "def accuracy(params, data_loader):\n",
    "    \"\"\" Compute the accuracy for a provided dataloader \"\"\"\n",
    "    acc_total = 0\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        images = jnp.array(data).reshape(data.size(0), 28*28)\n",
    "        targets = one_hot(jnp.array(target), num_classes)\n",
    "\n",
    "        target_class = jnp.argmax(targets, axis=1)\n",
    "        predicted_class = jnp.argmax(batch_forward(params, images), axis=1)\n",
    "        acc_total += jnp.sum(predicted_class == target_class)\n",
    "    return acc_total/len(data_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def update(params, x, y, opt_state):\n",
    "    \"\"\" Compute the gradient for a batch and update the parameters \"\"\"\n",
    "    value, grads = value_and_grad(loss)(params, x, y)\n",
    "    opt_state = opt_update(0, grads, opt_state)\n",
    "    return get_params(opt_state), opt_state, value\n",
    "\n",
    "# Defining an optimizer in Jax\n",
    "step_size = 1e-3\n",
    "opt_init, opt_update, get_params = optimizers.adam(step_size)\n",
    "opt_state = opt_init(params)\n",
    "\n",
    "num_epochs = 10\n",
    "num_classes = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | T: 20.09 | Train A: 0.972 | Test A: 0.965\n",
      "Epoch 2 | T: 20.83 | Train A: 0.985 | Test A: 0.975\n",
      "Epoch 3 | T: 20.81 | Train A: 0.990 | Test A: 0.976\n",
      "Epoch 4 | T: 20.94 | Train A: 0.993 | Test A: 0.979\n",
      "Epoch 5 | T: 21.40 | Train A: 0.994 | Test A: 0.980\n",
      "Epoch 6 | T: 21.21 | Train A: 0.996 | Test A: 0.981\n",
      "Epoch 7 | T: 23.30 | Train A: 0.997 | Test A: 0.982\n",
      "Epoch 8 | T: 22.03 | Train A: 0.994 | Test A: 0.977\n",
      "Epoch 9 | T: 22.01 | Train A: 0.998 | Test A: 0.980\n",
      "Epoch 10 | T: 18.74 | Train A: 0.997 | Test A: 0.981\n"
     ]
    }
   ],
   "source": [
    "def run_mnist_training_loop(num_epochs, opt_state, net_type=\"MLP\"):\n",
    "    \"\"\" Implements a learning loop over epochs. \"\"\"\n",
    "    # Initialize placeholder for loggin\n",
    "    log_acc_train, log_acc_test, train_loss = [], [], []\n",
    "\n",
    "    # Get the initial set of parameters\n",
    "    params = get_params(opt_state)\n",
    "\n",
    "    # Get initial accuracy after random init\n",
    "    train_acc = accuracy(params, train_loader)\n",
    "    test_acc = accuracy(params, test_loader)\n",
    "    log_acc_train.append(train_acc)\n",
    "    log_acc_test.append(test_acc)\n",
    "\n",
    "    # Loop over the training epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            if net_type == \"MLP\":\n",
    "                # Flatten the image into 784 vectors for the MLP\n",
    "                x = jnp.array(data).reshape(data.size(0), 28*28)\n",
    "            elif net_type == \"CNN\":\n",
    "                # No flattening of the input required for the CNN\n",
    "                x = jnp.array(data)\n",
    "            y = one_hot(jnp.array(target), num_classes)\n",
    "            params, opt_state, loss = update(params, x, y, opt_state)\n",
    "            train_loss.append(loss)\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        train_acc = accuracy(params, train_loader)\n",
    "        test_acc = accuracy(params, test_loader)\n",
    "        log_acc_train.append(train_acc)\n",
    "        log_acc_test.append(test_acc)\n",
    "        print(\"Epoch {} | T: {:0.2f} | Train A: {:0.3f} | Test A: {:0.3f}\".format(epoch+1, epoch_time,\n",
    "                                                                    train_acc, test_acc))\n",
    "\n",
    "    return train_loss, log_acc_train, log_acc_test\n",
    "\n",
    "\n",
    "train_loss, train_log, test_log = run_mnist_training_loop(num_epochs,\n",
    "                                                          opt_state,\n",
    "                                                          net_type=\"MLP\")\n",
    "\n",
    "# # Plot the loss curve over time\n",
    "# from helpers import plot_mnist_performance\n",
    "# plot_mnist_performance(train_loss, train_log, test_log,\n",
    "#                        \"MNIST MLP Performance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aipy310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
